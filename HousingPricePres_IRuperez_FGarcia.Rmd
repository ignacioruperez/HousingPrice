---
title: "1st Assignment"
output: 
  html_document:
    toc: true
    toc_depth: 3
author: Ignacio Ruperez & Federico Garcia (team name Slowly). Ranking 1514. Score 0.13086
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(plyr)
library(dplyr)
library(moments)
library(glmnet)
library(caret)
library(stringr)
library(Matrix) 
library(Metrics) 
library(scales) 
library(e1071) 
library(corrplot) 
library(FSelector)

```

# Data Reading and preparation
First of all, we need to upload the data sets: training and test.

## Intro

```{r Load Data}

train = read.csv("train.csv")
test = read.csv("test.csv")

```

We check for duplicates and remove the ID Column, since it is not relevant for our analysis. 

```{r Check for duplicates}
length(unique(train$Id)) == nrow(train)
```

There are no duplicates so we remove Id column and we combine both datasets to make the data processing easier. We must also remove SalePrice (our target variable) since it is not present in the test set. 

```{r Remove the ID Column}
test1 = read.csv("test.csv")

train1 = read.csv("train.csv")
train2 = within(train, rm('Id', 'SalePrice'))
test = within(test, rm('Id'))

both = rbind(train2, test)

```

## Dealing with missing values

Our dataset is filled with many missing values, therefore, before we can build any predictive model we'll clean our data by filling in all NA's with appropriate values.

```{r NAs discovery}
na.cols <- which(colSums(is.na(both)) > 0)
sort(colSums(sapply(both[na.cols], is.na)), decreasing = TRUE)
paste('There are', length(na.cols), 'columns with missing values')
```

There are 34 columns with NA values. Let's find the best way of imputing them by analyzing each of these variables.

There are many variables that describe a single feature of each house (and that therefore are probably highly correlated). For example, there are 9 features for Basement, and 7 for Garage. 

### PoolQC

There are 2906 missing values for PoolQC. If we check how many houses have a pool by analysing the PoolArea variable, we see that only 13 do have a pool and only 3 have PoolArea but have an NA value. Therefore, it is not worth it to do further manipulations other than filling NA in PoolQC with "No".

```{r}

sum(both$PoolArea > 0)

both %>% 
  filter(PoolArea > 0) %>% 
  select(PoolQC, PoolArea)

both$PoolQC = factor(both$PoolQC, levels=c(levels(both$PoolQC), "No"))
both$PoolQC[is.na(both$PoolQC)] = "No"
```

Further in the analysis we will evaluate if we should create a dummy variable Pool: Yes/No if we see that this has an influence on the sale price. 

### MiscFeature

There are 2814 missing values of this variable. According to the data explanation, NA stands for none.

```{r}

both$MiscFeature = factor(both$MiscFeature,levels=c(levels(both$MiscFeature), "No"))
both$MiscFeature[is.na(both$MiscFeature)] = "No"

```

### Alley

According to data explanations, NA corresponds to no alley. 

```{r}

both$Alley = factor(both$Alley, levels=c(levels(both$Alley), "None"))
both$Alley[is.na(both$Alley)] = "None"

```

### Fence

According to data explanations, NA corresponds to no fence.

```{r}
both$Fence = factor(both$Fence, levels=c(levels(both$Fence), "No"))
both$Fence[is.na(both$Fence)] = "No"

```

### Fireplace

According to data explanations, NA corresponds to no fireplace.

```{r}

both$FireplaceQu = factor(both$FireplaceQu, levels=c(levels(both$FireplaceQu), "No"))
both$FireplaceQu[is.na(both$FireplaceQu)] = "No"

```

### LotFrontage

LotFrontage is the linear feet of street connected to property. Since there are many empty values, we may use LotArea to fill in these values. There seems to be a correlation between LotFrontage and LotArea, so we estimate LotFrontage from a linear regression with LotArea. 

```{r}

lotest3 = both %>%
  select(LotArea, LotFrontage) %>%
  filter(!is.na(LotFrontage))

cor(lotest3$LotArea, lotest3$LotFrontage)

# We calculate the quantiles to erase the outliers 
quantile(lotest3$LotArea, 0.95)
quantile(lotest3$LotFrontage, 0.95)

# We obtain the linear relation between LotArea and Lotest
lotest4 = lotest3 %>%
  group_by(LotArea, LotFrontage) %>%
  filter(LotArea < 16000, LotFrontage < 107)

cor(lotest4$LotArea, lotest4$LotFrontage)

f <- lm(data = lotest4, LotFrontage ~ LotArea) 

both$LotFrontage[is.na(both$LotFrontage)] <- both$LotArea[is.na(both$LotFrontage)] * f$coefficients[2] + f$coefficients[1]

```

### Garage

We check if missing values for features that describe garages are due to the fact that those houses do not have a garage. The variable that determines this is GarageType. So 157 NAs in GarageType can be transformed to none. We can do the same with GarageCars and GarageArea. 

We check if all 157 NAs in GarageType correspond to all NAs in GarageYrBlt, GarageFinish, GarageQual and GarageCond (159 each).

```{r}

both %>%
  group_by(GarageYrBlt, GarageType) %>%
  summarize(count = n()) %>%
  filter(is.na(GarageYrBlt), is.na(GarageType)) %>%
  .$count

both %>%
  group_by(GarageFinish, GarageType) %>%
  summarize(count = n()) %>%
  filter(is.na(GarageFinish), is.na(GarageType)) %>%
  .$count

both %>%
  group_by(GarageQual, GarageType) %>%
  summarize(count = n()) %>%
  filter(is.na(GarageQual), is.na(GarageType)) %>%
  .$count

both %>%
  group_by(GarageCond, GarageType) %>%
  summarize(count = n()) %>%
  filter(is.na(GarageCond), is.na(GarageType)) %>%
  .$count

```

So we may conclude that only two garages of each category do exist but are not correctly registered. We can therefore fill missing numeric values ("GarageYrBlt", "GarageArea", "GarageCars") with 0 and categoric ("GarageType", "GarageFinish", "GarageQual", "GarageCond") with "None". 

```{r}
# Categorial 
both$GarageType = factor(both$GarageType, levels=c(levels(both$GarageType), "No"))
both$GarageType[is.na(both$GarageType)] = "No"

both$GarageFinish = factor(both$GarageFinish, levels=c(levels(both$GarageFinish), "No"))
both$GarageFinish[is.na(both$GarageFinish)] = "No"

both$GarageQual = factor(both$GarageQual, levels=c(levels(both$GarageQual), "No"))
both$GarageQual[is.na(both$GarageQual)] = "No"

both$GarageCond = factor(both$GarageCond, levels=c(levels(both$GarageCond), "No"))
both$GarageCond[is.na(both$GarageCond)] = "No"

# Numerical 
both$GarageYrBlt[is.na(both$GarageYrBlt)] = 0
both$GarageArea[is.na(both$GarageArea)] = 0
both$GarageCars[is.na(both$GarageCars)] = 0

```

On the other hand, by taking a look at GarageYear stats, we see that the maximum year is 2207. Since this makes no sense, we will assume it means 2007.

```{r}
summary(both$GarageYrBlt)

both$GarageYrBlt[both$GarageYrBlt==2207] = 2007

```

### Basement features

There are 11 Basement features and the NAs in them are different. For Qual, Cond, Exposure, FinType1 and FinType 2 (the features with the highest number of NA), data description states that NA = no basement. We will fill NAs in numeric values with '0'. 

```{r}

both$BsmtQual = factor(both$BsmtQual, levels=c(levels(both$BsmtQual), "No"))
both$BsmtQual[is.na(both$BsmtQual)] = "No"

both$BsmtCond = factor(both$BsmtCond, levels=c(levels(both$BsmtCond), "No"))
both$BsmtCond[is.na(both$BsmtCond)] = "No"

#both$BsmtExposure = factor(both$BsmtExposure,levels=c(levels(both$BsmtExposure), "No"))
both$BsmtExposure[is.na(both$BsmtExposure)] = "No"

both$BsmtFinType1 = factor(both$BsmtFinType1, levels=c(levels(both$BsmtFinType1), "No"))
both$BsmtFinType1[is.na(both$BsmtFinType1)] = "No"

both$BsmtFinType2 = factor(both$BsmtFinType2, levels=c(levels(both$BsmtFinType2), "No"))
both$BsmtFinType2[is.na(both$BsmtFinType2)] = "No"

both$BsmtFinSF1[is.na(both$BsmtFinSF1)] = 0
both$BsmtFinSF2[is.na(both$BsmtFinSF2)] = 0
both$BsmtUnfSF[is.na(both$BsmtUnfSF)] = 0
both$TotalBsmtSF[is.na(both$TotalBsmtSF)] = 0
both$BsmtFullBath[is.na(both$BsmtFullBath)] = 0
both$BsmtHalfBath[is.na(both$BsmtHalfBath)] = 0

```

### Masonry
There is a category "None" so we'll assume that empty rows are None for categorical and 0 for numerical.  
```{r}

both$MasVnrType[is.na(both$MasVnrType)] = "None"
both$MasVnrArea[is.na(both$MasVnrArea)] = 0

```

### All the remaining 8 columns

```{r}

#MSZoning: we have four NA left so we will substitute them for the most common category (RL).

both %>%
  group_by(MSZoning) %>%
  summarise(counter = n())

both$MSZoning[is.na(both$MSZoning)] = "RL"


#Utilities: since all utilities are type AllPub (this means "All Public Utilities") except one, we will drop this column

both = both %>% 
  select(-Utilities)

#Functional: we have 2 NA left that we will substitute by the most frequent category (Typ)

both %>%
  group_by(Functional) %>%
  summarise(counter = n())

both$Functional[is.na(both$Functional)] = "Typ"

#Exterior1st: we will substitute the remainer with most frequent value (Vinylsd)

both %>%
  group_by(Exterior1st) %>%
  summarise(counter = n())

both$Exterior1st[is.na(both$Exterior1st)] = "VinylSd"

# Exterior2nd: we do the same as in the previous category

both %>%
  group_by(Exterior2nd) %>%
  summarise(counter = n())

both$Exterior2nd[is.na(both$Exterior2nd)] = "VinylSd"

# Electrical: we substitute NA by the most common category 

both %>%
  group_by(Electrical) %>%
  summarise(counter = n())

both$Electrical[is.na(both$Electrical)] = "SBrkr"

# KitchenQual

both %>%
  group_by(KitchenQual) %>%
  summarise(counter = n())

both$KitchenQual[is.na(both$KitchenQual)] = "TA"

# SaleType

both %>%
  group_by(SaleType) %>%
  summarise(counter = n())

both$SaleType[is.na(both$SaleType)] = "WD"

```

```{r}

na.cols <- which(colSums(is.na(both)) > 0)
paste('There are now', length(na.cols), 'columns with missing values')

```

So we have no more NA columns

## Other data preparation

### Factorize features

```{r}

both$MSSubClass <- as.factor(both$MSSubClass)
both$MoSold <- as.factor(both$MoSold)

```

### Create new feature: Total Surface

Surface of a house is probably the most important variable when determining its price. Currently we do not have a Surface variable (it is splitted into two). So we create one with the sum of surface from basement, 1st floor and 2nd floor.

```{r}

both$TotalSF = both$TotalBsmtSF + both$X1stFlrSF + both$X2ndFlrSF

```

### Create new feature: New, remodeled or old house

Another variable that usually determines the price of a house is if it is first (new) or second hand, and if second hand, if it has been recently built or remodeled or not. Looking at the data, we are missing these variables. 

The only variable related to this is a variable called YearRemodAdd that indicates the remodel date and is the same as construction date if no remodeling or additions have been made. 

Therefore, we can create the following variables:
- ageofhouse
- yearsfromlastRemod

AGE OF HOUSE: to see how old the house is and NEWLY BUILT HOUSE: dummy to determine if the house is newly built

```{r}

both = both %>% 
  mutate(Age = YrSold - YearBuilt, NewHouse = ifelse(Age <= 1, 1, 0))

```

YEARSLASTREMOD and RECENTLYREMOD

```{r}

both = both %>% 
  mutate(Yearslastremod = YrSold - YearRemodAdd,
         Recentlyremod = ifelse(Yearslastremod < 1, 1, 0)) 

```

### Create new feature: Expensive and non expensive neighbourhoods

Where the house is located is another variable that usually has a big impact on the Sale Price. Currently, Neighborhoods are categorical variables that will not be included in our model. 

```{r}
#We check differences in mean and median prices per area: 

PriceAreas =train1 %>%
  select(Neighborhood, SalePrice) %>%
  group_by(Neighborhood) %>%
  summarise(median.price = median(SalePrice, na.rm = T), mean.price = mean(SalePrice)) %>%
  arrange(desc(mean.price))

PriceAreas

# We can create four types of areas (Rich, Med, Poor) and create dummies. Or we can create a dummy for those houses that are int the top5 more expensive areas. 

train1[,c('Neighborhood','SalePrice')] %>%
  group_by(Neighborhood) %>%
  summarise(median.price = median(SalePrice, na.rm = TRUE)) %>%
  arrange(median.price) %>%
  mutate(nb = factor(Neighborhood, levels=Neighborhood)) %>%
  ggplot(aes(x=nb, y=median.price)) +
  geom_point() +
  geom_text(aes(label = median.price, angle = 45), vjust = 2) +
  theme_minimal() +
  labs(x='Neighborhood', y='Median price') +
  theme(text = element_text(size=12),
        axis.text.x = element_text(angle=45))


# According to the graph, we can differentiate four different types of price range.

nb_range <- c('MeadowV' = 'D', 'IDOTRR' = 'D', 'Sawyer' = 'D', 'BrDale' = 'C', 'OldTown' = 'C', 'Edwards' = 'C', 'BrkSide' = 'C', 'Blueste' = 'C', 'SWISU' = 'C', 'NAmes' = 'C', 'NPkVill' = 'C', 'Mitchel' = 'C', 'SawyerW' = 'B', 'Gilbert' = 'B', 'NWAmes' = 'B', 'Blmngtn' = 'B', 'CollgCr' = 'B', 'ClearCr' = 'B', 'Crawfor' = 'B', 'Veenker' = 'B', 'Somerst' = 'B', 'Timber' = 'B', 'StoneBr' = 'A', 'NoRidge' = 'A', 'NridgHt' = 'A')

nbD = c('MeadowV', 'IDOTRR', 'Sawyer')
nbC = c('BrDale', 'OldTown', 'Edwards', 'BrkSide', 'Blueste', 'SWISU', 'NAmes', 'NPkVill', 'Mitchel')
nbB = c('SawyerW', 'Gilbert', 'NWAmes', 'Blmngtn', 'CollgCr', 'ClearCr', 'Crawfor', 'Veenker', 'Somerst', 'Timber')
nbA = c('StoneBr', 'NoRidge', 'NridgHt')


both = both %>%
  mutate(NbType = ifelse(Neighborhood %in% nbD, 'D', ifelse(Neighborhood %in% nbC, 'C', ifelse (Neighborhood %in% nbB, 'B', ifelse (Neighborhood %in% nbA, 'A', "F")))))

both$NbType = as.factor(both$NbType)

```

### Transform categorical values to numeric

```{r}

num_features <- names(which(sapply(both, is.numeric)))
cat_features <- names(which(sapply(both, is.character)))

both.numeric <- both[num_features]

group.both <- both[1:1460,]
group.both$SalePrice <- train$SalePrice

# Function that groups a column by its features and returns the median sale price for each unique feature

group.prices <- function(col) {
  group.table <- group.both[,c(col, 'SalePrice', 'OverallQual')] %>%
    group_by_(col) %>%
    summarise(mean.Quality = round(mean(OverallQual),2),
      mean.Price = mean(SalePrice), n = n()) %>%
    arrange(mean.Quality)
    
  print(qplot(x=reorder(group.table[[col]], -group.table[['mean.Price']]), y=group.table[['mean.Price']]) +
    geom_bar(stat='identity', fill='cornflowerblue') +
    theme_minimal() +
    scale_y_continuous(labels = dollar) +
    labs(x=col, y='Mean SalePrice') +
    theme(axis.text.x = element_text(angle = 45)))
  
  return(data.frame(group.table))
}

## Function to compute the mean overall quality for each quality

quality.mean <- function(col) {
  group.table <- both[,c(col, 'OverallQual')] %>%
    group_by_(col) %>%
    summarise(mean.qual = mean(OverallQual)) %>%
    arrange(mean.qual)
  
  return(data.frame(group.table))
}

# Function that maps a categoric value to its corresponding numeric value and returns that column to the data frame

map.fcn <- function(cols, map.list, df){
  for (col in cols){
    df[col] <- as.numeric(map.list[both[,col]])
  }
  return(df)
}

qual.cols <- c('ExterQual', 'ExterCond', 'GarageQual', 'GarageCond', 'FireplaceQu', 'KitchenQual', 'HeatingQC', 'BsmtQual')

group.prices('FireplaceQu')

group.prices('BsmtQual')

group.prices('KitchenQual')

qual.list <- c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)

both.numeric <- map.fcn(qual.cols, qual.list, both.numeric)

group.prices('BsmtFinType1')

# visualization for BsmtFinTyp2 instead of another table

both[,c('BsmtFinType1', 'BsmtFinSF1')] %>%
  group_by(BsmtFinType1) %>%
  summarise(medianArea = median(BsmtFinSF1), counts = n()) %>%
  arrange(medianArea) %>%
  ggplot(aes(x=reorder(BsmtFinType1,-medianArea), y=medianArea)) +
  geom_bar(stat = 'identity', fill='cornflowerblue') +
  labs(x='BsmtFinType2', y='Median of BsmtFinSF2') +
  geom_text(aes(label = sort(medianArea)), vjust = -0.5) +
  scale_y_continuous(limits = c(0,850)) +
  theme_minimal()

bsmt.fin.list <- c('None' = 0, 'Unf' = 1, 'LwQ' = 2,'Rec'= 3, 'BLQ' = 4, 'ALQ' = 5, 'GLQ' = 6)
both.numeric <- map.fcn(c('BsmtFinType1','BsmtFinType2'), bsmt.fin.list, both.numeric)

group.prices('Functional')

functional.list <- c('None' = 0, 'Sal' = 1, 'Sev' = 2, 'Maj2' = 3, 'Maj1' = 4, 'Mod' = 5, 'Min2' = 6, 'Min1' = 7, 'Typ'= 8)

both.numeric['Functional'] <- as.numeric(functional.list[both$Functional])

group.prices('GarageFinish')

garage.fin.list <- c('None' = 0,'Unf' = 1, 'RFn' = 1, 'Fin' = 2)

both.numeric['GarageFinish'] <- as.numeric(garage.fin.list[both$GarageFinish])

group.prices('Fence')

fence.list <- c('None' = 0, 'MnWw' = 1, 'GdWo' = 1, 'MnPrv' = 2, 'GdPrv' = 4)

both.numeric['Fence'] <- as.numeric(fence.list[both$Fence])

MSdwelling.list <- c('20' = 1, '30'= 0, '40' = 0, '45' = 0,'50' = 0, '60' = 1, '70' = 0, '75' = 0, '80' = 0, '85' = 0, '90' = 0, '120' = 1, '150' = 0, '160' = 0, '180' = 0, '190' = 0)

both.numeric['NewerDwelling'] <- as.numeric(MSdwelling.list[as.character(both$MSSubClass)])

```

# Correlation

```{r}

# We need the SalePrice column
corr.train <- cbind(both.numeric[1:1460,], train['SalePrice'])

# Only using the first 1460 rows - training data
correlations <- cor(corr.train)

# Only want the columns that show strong correlations with SalePrice
corr.SalePrice <- as.matrix(sort(correlations[,'SalePrice'], decreasing = TRUE))

corr.idx <- names(which(apply(corr.SalePrice, 1, function(x) (x > 0.5 | x < -0.5))))

corrplot(as.matrix(correlations[corr.idx,corr.idx]), type = 'upper', method='color', addCoef.col = 'black', tl.cex = .7,cl.cex = .7, number.cex=.7)

```

# Redundant variables

We delete redundant variables and the ones with correlation to SalesPrice close to zero

```{r warning=FALSE}

# Neighborhood,TotalBsmtSF, X1stFlrSF, X2ndFlrSF, YearRemodAdd 

both = within(both, rm('Neighborhood', 'TotalBsmtSF', 'X1stFlrSF', 'X2ndFlrSF', 'YearRemodAdd'))

# We delete low correlated variables

both = within(both, rm('ExterCond', 'Street', 'MoSold', 'X3SsnPorch', 'BsmtHalfBath', 'MiscVal', 'YrSold'))

```

# Skewness for variables

To facilitate the application of the regression model we are going to eliminate skewness.
For numeric feature with excessive skewness, we perform log transformation

```{r}

column_types <- sapply(names(both),function(x){class(both[[x]])})
numeric_columns <-names(column_types[column_types != "factor"])

# Skew of each variable
skew <- sapply(numeric_columns,function(x){skewness(both[[x]],na.rm = T)})

# We transform all variables above a threshold skewness.
skew <- skew[skew > 0.75]
for(x in names(skew)) {
  both[[x]] <- log(both[[x]] + 1)
}
```

# Dataset separation

```{r}

training_data = cbind(both[1:1460,], train1["SalePrice"])
test_data = cbind(test1["Id"], both[1461:2919,])

```

# Skewness for Price

Skewness in the Target value

```{r}
# We get data frame of SalePrice and log(SalePrice + 1) for plotting
df <- rbind(data.frame(version="log(price+1)",x=log(training_data$SalePrice + 1)),
            data.frame(version="price",x=training_data$SalePrice))

ggplot(data=df) +
  facet_wrap(~version,ncol=2,scales="free_x") +
  geom_histogram(aes(x=x), bins = 50)
```

We therefore transform the target value applying log

```{r Log transform the target for official scoring}
# Log transform the target for official scoring
training_data$SalePrice <- log1p(training_data$SalePrice)
```

# Train and Validation Spliting

We are going to split the annotated dataset in training and validation for the later evaluation of our regression models

```{r Train test split}

splitdf <- function(dataframe, seed=NULL) {
  if (!is.null(seed)) set.seed(seed)
 	index <- 1:nrow(dataframe)
 	trainindex <- sample(index, trunc(length(index)/1.5))
 	trainset <- dataframe[trainindex, ]
 	testset <- dataframe[-trainindex, ]
 	list(trainset=trainset,testset=testset)
}
splits <- splitdf(training_data, seed=1)
training <- splits$trainset
validation <- splits$testset
```

# Modelling

We will rank the features according to their predictive power according to this methodologies: the Chi Squared Independence test and the Information Gain.

## Full Model

We must always fit a lm model with all the features to have a baseline to evaluate the impact of the feature engineering. We will place all this code in a function, so we can call it again and again without having to re-write everything. The only thing that changes from one case to another is the dataset that is used to train the model.

```{r Full Regression model, message=FALSE, warning=FALSE}

lm.model <- function(training_dataset, title) {
  set.seed(121)
  train_control_config <- trainControl(method = "repeatedcv", 
                                       number = 5, 
                                       repeats = 1,
                                       returnResamp = "all")
  
  this.model <- train(SalePrice ~ ., 
                       data = training_dataset, 
                       method = "lm", 
                       metric = "RMSE",
                       preProc = c("center", "scale"),
                       trControl=train_control_config)
  
  for (x in names(validation)) {
    this.model$xlevels[[x]] <- union(this.model$xlevels[[x]], levels(validation[[x]]))
  }
  this.model.pred <- predict(this.model, validation[,-ncol(validation)])
  this.model.pred[is.na(this.model.pred)] <- 0
  
  my_data <- as.data.frame(cbind(predicted=this.model.pred, observed=validation$SalePrice))
  thismodel.rmse <- sqrt(mean((this.model.pred - validation$SalePrice)^2))
  
  ggplot(my_data, aes(predicted, observed)) +
    geom_point() + geom_smooth(method = "lm") +
    labs(x="Predicted") +
    ggtitle(ggtitle(paste(title, ': ', format(round(thismodel.rmse, 4), nsmall=4), '; Nr. of features: ', 
                          length(training_dataset), sep='')))  
}
```

And now, we call the function with all features

```{r}
lm.model(training, "Baseline")
```

## Chi-squared Selection

```{r warning=FALSE}

features <- names(training[, sapply(training, is.factor) & colnames(training) != 'SalePrice'])
chisquared <- data.frame(features, statistic = sapply(features, function(x) {
  chisq.test(training$SalePrice, training[[x]])$statistic
}))

par(mfrow=c(1,2))
boxplot(chisquared$statistic)
bp.stats <- as.integer(boxplot.stats(chisquared$statistic)$stats)

chisquared.threshold = bp.stats[2]
text(y = bp.stats, labels = bp.stats, x = 1.3, cex=0.7)
barplot(sort(chisquared$statistic), names.arg = chisquared$features, cex.names = 0.6, las=2, horiz = T)
abline(v=chisquared.threshold, col='red')
```

Now, we can test if this is a good move, by removing any feature with a Chi Squared test statistic against the output below the 1 IQR.

```{r warning=FALSE}
# We determine what features to remove from the training set.
features_to_remove <- as.character(chisquared[chisquared$statistic < chisquared.threshold, "features"])
lm.model(training[!names(training) %in% features_to_remove], "ChiSquared Model")
```

Our model gets worse, so we keep all the current variables.

## Spearman's correlation.

```{r warning=FALSE}
features <- names(training[, sapply(training, is.numeric) & colnames(training) != 'SalePrice'])

spearman <- data.frame(features, statistic = sapply(features, function(x) {
  cor(training$SalePrice, training[[x]], method='spearman')
}))

par(mfrow=c(1,2))
boxplot(abs(spearman$statistic))
bp.stats <- boxplot.stats(abs(spearman$statistic))$stats
text(y = bp.stats, 
     labels = sapply(bp.stats, function(x){format(round(x, 3), nsmall=3)}),
     x = 1.3, cex=0.7)

spearman.threshold = bp.stats[2]

barplot(sort(abs(spearman$statistic)), names.arg = spearman$features, cex.names = 0.6, las=2, horiz = T)
abline(v=spearman.threshold, col='red')
```

So, how good is our feature cleaning process? Let's train the model with the new features, exactly as we did in the Chi Sq. section above.

```{r warning=FALSE}
# Determine what features to remove from the training set.
features_to_remove <- as.character(spearman[spearman$statistic < spearman.threshold, "features"])
lm.model(training[!names(training) %in% features_to_remove], "Spearman's Correlation")
```

The model also gets a worse result, so we continue keeping all the current variables.

## Information Gain Selection

We experiment now with Information Gain Selection. This part is equivalent to the Chi Squared, but with another metric.

```{r warning=FALSE}

weights<- data.frame(information.gain(SalePrice~., training_data))
weights$feature <- rownames(weights)
weights[order(weights$attr_importance, decreasing = TRUE),]

information_gain_features <- weights$feature[weights$attr_importance >= 0.05]

```

### Evaluation

We evaluate the impact of the IG selection in the model performance

```{r Information Gain Regression Model, message=FALSE, warning=FALSE}

train_control_config <- trainControl(method = "repeatedcv", 
                       number = 5, 
                       repeats = 1,
                       returnResamp = "all")

ig.lm.mod <- train(SalePrice ~ ., data = training[append(information_gain_features, "SalePrice")], 
               method = "lm", 
               metric = "RMSE",
               preProc = c("center", "scale"),
               trControl=train_control_config)

for (x in names(validation)) {
  ig.lm.mod$xlevels[[x]] <- union(ig.lm.mod$xlevels[[x]], levels(validation[[x]]))
}
ig.lm.mod.pred <- predict(ig.lm.mod, validation[,-ncol(validation)])
ig.lm.mod.pred[is.na(ig.lm.mod.pred)] <- 0

my_data=as.data.frame(cbind(predicted=ig.lm.mod.pred,observed=validation$SalePrice))

ggplot(my_data,aes(predicted,observed))+
  geom_point() + geom_smooth(method = "lm") +
  labs(x="Predicted") +
  ggtitle(paste('Nr. of features:', length(information_gain_features), sep=''))  

paste("IG Filtered Linear Regression RMSE = ", sqrt(mean((ig.lm.mod.pred - validation$SalePrice)^2)))
```

Based on these results, we filter the training and validation set with the Information Gain features.

```{r warning=FALSE}

training <- training[append(information_gain_features, "SalePrice")]
validation <- validation[append(information_gain_features, "SalePrice")]

```

## Ridge 

```{r}
lambdas <- 10^seq(-2, 3, by = .1)
ridge.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 0, lambda = lambdas)

RMSE = numeric(length(lambdas))
for (i in seq_along(lambdas)){
  ridge.pred=predict(ridge.mod, s=lambdas[i], data.matrix(validation[,-ncol(validation)]))
  RMSE[i] <- sqrt(mean((ridge.pred - validation$SalePrice)^2))
}
plot(lambdas, RMSE, main="Ridge", log="x", type = "b")


ridge.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 0, lambda = lambdas)
plot(ridge.cv_fit)


bestlam <- ridge.cv_fit$lambda.1se
paste("Best Lambda value from CV =", bestlam)

ridge.pred <- predict(ridge.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", bestlam, " = ", sqrt(mean((ridge.pred - validation$SalePrice)^2)))

my_data <- as.data.frame(cbind(predicted=ridge.pred, observed=validation$SalePrice))

ggplot(my_data,aes(my_data["1"],observed))+
  geom_point()+geom_smooth(method="lm")+
  scale_x_continuous(expand = c(0,0)) +
  labs(x="Predicted") +
  ggtitle('Ridge')

imp <- varImp(ridge.mod, lambda = bestlam)
names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)]
importance <- imp[names,]

data.frame(row.names = names, importance)


```

## Lasso 

```{r}
lambdas <- 10^seq(-3, 3, by = .1)

lasso.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
plot(lasso.cv_fit)

bestlam <- lasso.cv_fit$lambda.min
paste("Best Lambda value from CV=", bestlam)

lasso.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
lasso.pred=predict(lasso.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", bestlam, " = ", sqrt(mean((lasso.pred - validation$SalePrice)^2)))

lam1se <- lasso.cv_fit$lambda.1se
paste("Lambda 1se value from CV=", lam1se)

lasso.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
lasso.pred=predict(lasso.mod, s=lam1se, data.matrix(validation[,-ncol(validation)]))
paste("RMSE for lambda ", lam1se, " = ", sqrt(mean((lasso.pred - validation$SalePrice)^2)))

my_data=as.data.frame(cbind(predicted=lasso.pred,observed=validation$SalePrice))

ggplot(my_data,aes(my_data["1"],observed))+
  geom_point()+geom_smooth(method="lm")+
  scale_x_continuous(expand = c(0,0)) +
  labs(x="Predicted") +
  ggtitle('Lasso')

imp <- varImp(lasso.mod, lambda = bestlam)
names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)]
importance <- imp[names,]

data.frame(row.names = names, importance)

```

We see that Lasso produces the best results, as it provides the lowest RMSE. 
We have observed that RMSE varies significantly depending on the seed number passed to the split function, so we are looping through a number of different seeds to get the best result.
We are also checking if different combinations of the 47 gain features provide a better result (we will do that looping for samples between 30 and 46 features.)
We will stop the loops when they find a result with a RMSE lower than 0.113, and use those features to predict the prices for the test dataset and create the result.csv file to submit to Kaggle.

```{r}
results <- data.frame(matrix(ncol = 2, nrow = 0))

seed_number <- 1
final_rmse <- 0.15

while(final_rmse >= 0.113){
  seed_number <- seed_number + 1
  set.seed(seed_number)
  print(paste("Seed_number:", seed_number))
  for(j in 30:46){
    information_gain_features2 <- sample(information_gain_features, j)
    splits <- splitdf(training_data)
    training <- splits$trainset
    validation <- splits$testset
    training <- training[append(information_gain_features2, "SalePrice")]
    validation <- validation[append(information_gain_features2, "SalePrice")]
    
    lambdas <- 10^seq(-3, 3, by = .1)
    
    lasso.cv_fit <- cv.glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
    
    bestlam <- lasso.cv_fit$lambda.min
    
    lasso.mod <- glmnet(x = data.matrix(training[,-ncol(training)]), y=training$SalePrice, alpha = 1, lambda = lambdas)
    lasso.pred=predict(lasso.mod, s=bestlam, data.matrix(validation[,-ncol(validation)]))
    RMSElambdamin <- sqrt(mean((lasso.pred - validation$SalePrice)^2))
    
    final_rmse <- RMSElambdamin
    
    print(paste("RMSE: ", final_rmse))
    
    information_gain_features3 <- paste(information_gain_features2, collapse = " ")
    new_line <- c(information_gain_features3, RMSElambdamin)
    results <- rbind(results, new_line, stringsAsFactors = FALSE)
    
    if(final_rmse < 0.113){
      break
    }
  }
}
x <- c("Features", "Min")
colnames(results) <- x

write.csv(results,file="results.csv",row.names=F)

log_prediction <- predict(lasso.mod,  s=lasso.cv_fit$lambda.min, newx = data.matrix(test_data[information_gain_features2]))
actual_pred <- exp(log_prediction)-1
hist(actual_pred)

my_data=as.data.frame(cbind(predicted=lasso.pred,observed=validation$SalePrice))

ggplot(my_data,aes(my_data["1"],observed))+
  geom_point()+geom_smooth(method="lm")+
  scale_x_continuous(expand = c(0,0)) +
  labs(x="Predicted") +
  ggtitle('Lasso')

imp <- varImp(lasso.mod, lambda = bestlam)
names <- rownames(imp)[order(imp$Overall, decreasing=TRUE)]
importance <- imp[names,]

data.frame(row.names = names, importance)

submit <- data.frame(Id=test_data$Id,SalePrice=actual_pred)
colnames(submit) <-c("Id", "SalePrice")

submit$SalePrice[is.na(submit$SalePrice)] <- 0
replace_value_for_na <- sum(na.omit(submit$SalePrice))/(nrow(submit) - sum(submit$SalePrice == 0))
submit$SalePrice[submit$SalePrice == 0] <- replace_value_for_na

write.csv(submit,file="result.csv",row.names=F)

```
